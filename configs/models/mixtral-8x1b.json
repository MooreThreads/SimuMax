{
    "model_type":"moe",
    "model_name":"mixtral_8x1b",
    "hidden_size": 4096,
    "head_num": 32,
    "kv_head_num": 8,
    "head_size": 128,
    "intermediate_size": 14336,
    "layer_num": 4,
    "expert_num": 8,
    "topk":2,
    "vocab_size": 32000,
    "use_swiglu": true
}