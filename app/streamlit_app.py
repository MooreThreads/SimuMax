from functools import partial
import streamlit as st
import re
import os
import copy
os.environ['SIMU_CHECK'] = '1'
import json
import zipfile
from datetime import datetime
from io import BytesIO
from simumax.core.config import ParameterExtractor
from simumax.core.utils import HumanReadableSize
from simumax.core.perf_llm import PerfLLM
from simumax.core.config import ModelConfig, StrategyConfig, SystemConfig

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="SimuMaxåˆ†æå·¥å…·",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)
perf_model = PerfLLM()

class ParameterAnalyzer:
    def __init__(self):
        # å¯é€‰çš„é…ç½®é¢„è®¾
        self.config_presets = {
            'small': {
                'seq_len': 512,
                'micro_batch_size': 8,
                'micro_batch_num': 2,
                'global_batch_size': 16,
                'tp_size': 2,
                'ep_size': 1,
                'pp_size': 2,
                'dp_size': 2,
                'world_size': 4
            },
            'medium': {
                'seq_len': 1024,
                'micro_batch_size': 16,
                'micro_batch_num': 4,
                'global_batch_size': 64,
                'tp_size': 4,
                'ep_size': 1,
                'pp_size': 2,
                'dp_size': 4,
                'world_size': 32
            },
            'large': {
                'seq_len': 2048,
                'micro_batch_size': 32,
                'micro_batch_num': 8,
                'global_batch_size': 256,
                'tp_size': 8,
                'ep_size': 1,
                'pp_size': 4,
                'dp_size': 8,
                'world_size': 256
            }
        }
        
        # ç¡¬ä»¶é…ç½®é€‰é¡¹
        from simumax.utils import RELEASE_MODELS,RELEASE_SYSTEM
        self.simumax_hardware_options = {
            'A100-80GB-PCIE': RELEASE_SYSTEM['a100_pcie'],
        }
        # æ¨¡å‹è§„æ¨¡é€‰é¡¹
        self.simumax_model_options = RELEASE_MODELS
        self.simumax_model_options = {
            'deepseek_v2': RELEASE_MODELS['deepseekv2'],
            'deepseek_v3': RELEASE_MODELS['deepseekv3'],
            'llama3_8b': RELEASE_MODELS['llama3-8b'],
            'llama3_70b': RELEASE_MODELS['llama3-70b'],
            'llama3_405b': RELEASE_MODELS['llama3-405b_padding_128'],
        }

    def analyze_parameters(self, params, hardware_name, model_name):
        """åˆ†æå‚æ•°å¹¶è¿”å›ç»“æœ"""
        hardware = self.hardware_options[hardware_name]
        model = self.model_options[model_name]
        
        # å‚æ•°éªŒè¯
        warnings = []
        recommendations = []
        
        # è®¡ç®—ç†è®ºå€¼
        calculated_world_size = params['tp_size'] * params['pp_size'] * params['dp_size']
        actual_global_batch_size = params['micro_batch_size'] * params['micro_batch_num'] * params['dp_size']
        
        # å†…å­˜ä¼°ç®— (ç®€åŒ–æ¨¡å‹)
        activation_memory = (params['seq_len'] * params['micro_batch_size'] * 
                           params['tp_size'] * 4 * 12) / (1024 ** 3)  # GB
        
        model_memory = (model['params'] * 4) / (1024 ** 3)  # å‡è®¾ FP32, GB
        
        total_memory_estimate = activation_memory + model_memory
        
        # æ£€æŸ¥å†…å­˜æ˜¯å¦è¶³å¤Ÿ
        if total_memory_estimate > hardware['memory'] * 0.9:  # ç•™10%ä½™é‡
            warnings.append(f"å†…å­˜ä¼°è®¡ {total_memory_estimate:.2f}GB è¶…è¿‡ç¡¬ä»¶é™åˆ¶ {hardware['memory']}GB")
            recommendations.append("è€ƒè™‘å‡å°æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ")
        
        # æ£€æŸ¥é…ç½®ä¸€è‡´æ€§
        if params['world_size'] != calculated_world_size:
            warnings.append(f"world_size é…ç½®ä¸ä¸€è‡´: è¾“å…¥å€¼ {params['world_size']}, è®¡ç®—å€¼ {calculated_world_size}")
            recommendations.append(f"å»ºè®®å°† world_size è®¾ç½®ä¸º {calculated_world_size}")
        
        if params['global_batch_size'] != actual_global_batch_size:
            warnings.append(f"global_batch_size é…ç½®ä¸ä¸€è‡´: è¾“å…¥å€¼ {params['global_batch_size']}, è®¡ç®—å€¼ {actual_global_batch_size}")
            recommendations.append(f"å»ºè®®å°† global_batch_size è®¾ç½®ä¸º {actual_global_batch_size}")
        
        # æ€§èƒ½ä¼°ç®—
        communication_overhead = (params['tp_size'] + params['pp_size']) * 0.05
        efficiency_score = max(0, 100 - communication_overhead * 100)
        
        # ååé‡ä¼°ç®—
        estimated_throughput = (params['global_batch_size'] / 
                              (1 + communication_overhead))  # tokens/step
        
        return {
            'parameters': params,
            'analysis': {
                'calculated_world_size': calculated_world_size,
                'actual_global_batch_size': actual_global_batch_size,
                'memory_estimate_gb': round(total_memory_estimate, 2),
                'activation_memory_gb': round(activation_memory, 2),
                'model_memory_gb': round(model_memory, 2),
                'efficiency_score': round(efficiency_score, 1),
                'estimated_throughput': round(estimated_throughput, 2),
                'communication_overhead': round(communication_overhead, 2),
                'hardware_utilization': round((total_memory_estimate / hardware['memory']) * 100, 1),
                'warnings': warnings,
                'recommendations': recommendations,
                'is_config_valid': len(warnings) == 0
            },
            'hardware_info': {'name': hardware_name, **hardware},
            'model_info': {'name': model_name, **model}
        }

def create_download_zip(perf_model:PerfLLM, mem_result, compute_result):
    """åˆ›å»ºä¸‹è½½æ–‡ä»¶"""
    # åˆ›å»ºå†…å­˜zipæ–‡ä»¶
    memory_file = BytesIO()
        
    with zipfile.ZipFile(memory_file, 'w') as zf:
        self = perf_model
        base_info = {}
        base_info["arch"] = str(self.model_chunk_dict)
        base_info["all_param"] = self.model_config.param_numel
        base_info["act_param"] = self.model_config.activated_param_numel
        # æ·»åŠ æ–‡æœ¬æŠ¥å‘Š
        zf.writestr('model_arch.txt', base_info["arch"])
        # æ·»åŠ JSONé…ç½®
        zf.writestr('base_info.json', json.dumps(base_info, indent=2, sort_keys=False, ensure_ascii=False))
        zf.writestr('mem_result.json', str(mem_result))
        zf.writestr('compute_result.json', str(compute_result))
        zf.writestr('strategy_config.json', str(self.strategy))
        zf.writestr('system_config.json', str(self.system))
        zf.writestr('model_config.json', str(self.model_config))
    
    memory_file.seek(0)
    return memory_file

def main():
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = ParameterAnalyzer()
    
    # é¡µé¢æ ‡é¢˜
    st.title("ğŸš€ SimuMaxåˆ†æå·¥å…·")
    st.markdown("åˆ†æå’Œä¼˜åŒ–åˆ†å¸ƒå¼è®­ç»ƒé…ç½®å‚æ•°")
    
    # ä¾§è¾¹æ  - å¿«é€Ÿé…ç½®
    with st.sidebar:
        st.header("âš¡ å¿«é€Ÿé…ç½®")

        if 'selected_hardware' not in st.session_state:
            st.session_state.selected_hardware = "A100-80GB-PCIE"
        if 'selected_model' not in st.session_state:
            st.session_state.selected_model = "deepseek_v3"

        def update_hardware(main=False):
            if main:
                st.session_state.selected_hardware = st.session_state.main_hardware
            else:
                st.session_state.selected_hardware = st.session_state.side_hardware
            

        def update_model(main=False):
            if main:
                st.session_state.selected_model = st.session_state.main_model
            else:
                st.session_state.selected_model = st.session_state.side_model
            

        def update_paralism():
            paralism = st.session_state.side_paralism
            param_patterns = {
                'tp_size': (r'TP(\d+)', 1),
                'ep_size': (r'EP(\d+)', 1),
                'pp_size': (r'PP(\d+)', 1),
                'world_size': (r'GPU(\d+)', 8),
            }
            paralism_params = ParameterExtractor(param_patterns=param_patterns).extract_parameters(paralism)
            print(paralism_params)
            for key, value in paralism_params.items():
                if key in st.session_state:
                    st.session_state[key] = value

        # ç¡¬ä»¶é€‰æ‹©
        side_hardware = st.selectbox(
            "é€‰æ‹©ç¡¬ä»¶é…ç½®",
            list(analyzer.simumax_hardware_options.keys()),
            index=list(analyzer.simumax_hardware_options.keys()).index(st.session_state.selected_hardware),
            key="side_hardware",
            on_change = partial(update_hardware, main=False),
        )
        
        # æ¨¡å‹é€‰æ‹©
        side_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹è§„æ¨¡",
            list(analyzer.simumax_model_options.keys()),
            key="side_model",
            index=list(analyzer.simumax_model_options.keys()).index(st.session_state.selected_model),
            on_change = partial(update_model, main=False),
        )
        init_paralisms = ['TP1+PP1+GPU8', 'TP1+PP2+GPU8', 'TP2+PP1+GPU8', 'EP4+PP2+GPU8', 'EP8+PP1+GPU8']
        side_paralism = st.selectbox(
            "é€‰æ‹©å¹¶è¡Œæ–¹å¼",
            init_paralisms,
            key="side_paralism",
            index=init_paralisms.index('EP8+PP1+GPU8'),
            on_change = update_paralism
        )

        st.markdown("---")
    
    
    # ç¡¬ä»¶é€‰æ‹©
    main_hardware = st.selectbox(
        "é€‰æ‹©ç¡¬ä»¶é…ç½®",
        # list(analyzer.hardware_options.keys())
        list(analyzer.simumax_hardware_options.keys()),
        index=list(analyzer.simumax_hardware_options.keys()).index(st.session_state.selected_hardware),
    )

    # æ¨¡å‹é€‰æ‹©
    main_model = st.selectbox(
        "é€‰æ‹©æ¨¡å‹è§„æ¨¡",
        # list(analyzer.model_options.keys())
        list(analyzer.simumax_model_options.keys()),
        index=list(analyzer.simumax_model_options.keys()).index(st.session_state.selected_model),
    )
    perf_model.configure(
        strategy_config=StrategyConfig.init_from_format_strings("gbs8"),
        model_config=ModelConfig.init_from_config_file(analyzer.simumax_model_options[main_model]),
        system_config=SystemConfig.init_from_config_file(analyzer.simumax_hardware_options[main_hardware])
    )
    st.success(f"âœ… å·²é€‰æ‹©: {main_model}/{main_hardware}")

    with st.expander("ğŸ“‹ æ¨¡å‹è¯¦ç»†ä¿¡æ¯", expanded=True):
        detail_col1, detail_col2, detail_col3 = st.columns(3)
        model_info = perf_model.model_config
        with detail_col1:
            st.write(f"**æ¨¡å‹ç±»å‹:** {model_info.model_type}")
            st.write(f"**æ¨¡å‹åç§°:** {model_info.model_name}")
            st.write(f"**æ³¨æ„åŠ›ç±»å‹:** {model_info.attention_type}")
            st.write(f"**éšè—å±‚å¤§å°:** {model_info.hidden_size}")
            st.write(f"**å¤´æ•°é‡:** {model_info.head_num}")
            st.write(f"**KVå¤´æ•°é‡:** {model_info.kv_head_num}")
        
        with detail_col2:
            st.write(f"**å¤´å¤§å°:** {model_info.head_size}")
            st.write(f"**ä¸­é—´éšè—å±‚å¤§å°:** {model_info.intermediate_size}")
            st.write(f"**æ€»å±‚æ•°:** {model_info.layer_num}")
            
            if model_info.model_type == 'moe':
                st.write(f"**ç¨ å¯†å±‚æ•°:** {model_info.dense_layers}")
                st.write(f"**ä¸“å®¶æ•°é‡:** {model_info.expert_num}")
                st.write(f"**TopK:** {model_info.topk}")
                st.write(f"**MoE FFNéšè—å±‚å¤§å°:** {model_info.moe_ffn_hidden_size}")
                st.write(f"**MoEå…±äº«ä¸“å®¶éšè—å±‚å¤§å°:** {model_info.moe_shared_expert_intermediate_size}")
        
        with detail_col3:
            if model_info.attention_type == 'mla':
                st.write(f"**Væ³¨æ„åŠ›å¤´ç»´åº¦:** {model_info.v_head_dim}")
                st.write(f"**QKæ³¨æ„åŠ›å¤´ç»´åº¦:** {model_info.qk_head_dim}")
                st.write(f"**Q LoRAç§©:** {model_info.q_lora_rank}")
                st.write(f"**KV LoRAç§©:** {model_info.kv_lora_rank}")
                st.write(f"**QKä½ç½®ç¼–ç ç»´åº¦:** {model_info.qk_pos_emb_head_dim}")
            
            st.write(f"**è¯è¡¨å¤§å°:** {model_info.vocab_size}")
            st.write(f"**ä½¿ç”¨SwiGLU:** {'æ˜¯' if model_info.use_swiglu else 'å¦'}")
    
    st.markdown("---")
    st.markdown("### æ“ä½œ")
    st.markdown("""
    <style>
        div[data-testid="stButton"] > button {
            background: linear-gradient(45deg, #3498db, #2980b9);
            color: white;
            border: none;
            padding: 15px 30px;
            font-size: 20px;
            font-weight: bold;
            border-radius: 12px;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px 0 rgba(52, 152, 219, 0.4);
            width: 100%;
        }
        div[data-testid="stButton"] > button:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px 0 rgba(52, 152, 219, 0.6);
            background: linear-gradient(45deg, #2980b9, #3498db);
        }
                
    </style>
    """, unsafe_allow_html=True)

    analyze_btn = st.button("ğŸš€ å¼€å§‹è¯„ä¼°é…ç½®", use_container_width=True)
    # analyze_btn = st.button("ğŸ¯ è¯„ä¼°é…ç½®", use_container_width=True)
    st.markdown("---")

    st.markdown("### å…³äº")
    st.info("""
    æœ¬å·¥å…·ç”¨äºåˆ†æåˆ†å¸ƒå¼è®­ç»ƒå‚æ•°é…ç½®ï¼Œ
    æä¾›å†…å­˜ä¼°ç®—ã€æ€§èƒ½è¯„ä¼°å’Œä¼˜åŒ–å»ºè®®ã€‚
    """)

    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.header("ğŸ–¥ï¸ ç¡¬ä»¶å‚æ•°é…ç½®")

        col_hw1, col_hw2, col_hw3 = st.columns(3)

        with col_hw1:
            st.subheader("ç½‘ç»œé€šä¿¡")
            if 'high_intra_node' in perf_model.system.networks:
                intra_network_bandwidth = st.number_input(
                    "æœºå†…ç½‘ç»œå¸¦å®½ (GB/s)",
                    min_value=0.1,
                    max_value=1000.0,
                    value=st.session_state.get('intra_network_bandwidth', float(perf_model.system.networks['high_intra_node'].bandwidth.gbps)),
                    step=0.1,
                    help="èŠ‚ç‚¹å†…ç½‘ç»œé€šä¿¡å¸¦å®½"
                )
            if 'inter_node' in perf_model.system.networks:
                inter_network_bandwidth = st.number_input(
                    "æœºé—´ç½‘ç»œå¸¦å®½ (GB/s)",
                    min_value=0.1,
                    max_value=1000.0,
                    value=st.session_state.get('inter_network_bandwidth', float(perf_model.system.networks['inter_node'].bandwidth.gbps)),
                    step=0.1,
                    help="èŠ‚ç‚¹é—´ç½‘ç»œé€šä¿¡å¸¦å®½"
                )

        with col_hw2:
            st.subheader("è®¡ç®—èƒ½åŠ›")
            compute_performance = st.number_input(
                "ç®—åŠ› (TFLOPS)",
                min_value=1.0,
                max_value=1000.0,
                value=float(perf_model.system.accelerator.op['matmul'].tflops),
                step=1.0,
                help="å•å¡ç†è®ºè®¡ç®—æ€§èƒ½"
            )

        # with col_hw3:
        #     st.subheader("ç®—å­æ•ˆç‡")
        #     st.write("æŒ‡å®šshapeä¸‹çš„è®¡ç®—æ•ˆç‡")
        #     op_efficiency_attn = st.slider(
        #         "æ³¨æ„åŠ›ç®—å­æ•ˆç‡ (%)",
        #         min_value=10,
        #         max_value=100,
        #         value=65,
        #         help="æ³¨æ„åŠ›è®¡ç®—åœ¨å®é™…shapeä¸‹çš„æ•ˆç‡"
        #     )
        #     op_efficiency_mlp = st.slider(
        #         "MLPç®—å­æ•ˆç‡ (%)", 
        #         min_value=10,
        #         max_value=100,
        #         value=75,
        #         help="MLPè®¡ç®—åœ¨å®é™…shapeä¸‹çš„æ•ˆç‡"
        #     )

        st.header("ğŸ“Š å‚æ•°é…ç½®")
        
        # å‚æ•°è¾“å…¥è¡¨æ ¼
        with st.container():
            st.subheader("è®­ç»ƒå‚æ•°")
            train_col1_1, train_col1_2 = st.columns(2)
            
            with train_col1_1:
                seq_len = st.number_input(
                    "åºåˆ—é•¿åº¦ (seq_len)",
                    min_value=1,
                    value=st.session_state.get('seq_len', 4096),
                    key='seq_len'
                )
                
                micro_batch_size = st.number_input(
                    "å¾®æ‰¹æ¬¡å¤§å° (mbs)",
                    min_value=1,
                    value=st.session_state.get('micro_batch_size', 1),
                    key='micro_batch_size'
                )
                
                global_batch_size = st.number_input(
                    "å…¨å±€æ‰¹æ¬¡å¤§å° (gbs)",
                    min_value=1,
                    value=st.session_state.get('global_batch_size', 256),
                    key='global_batch_size'
                )
             
                dtype = st.selectbox(
                    "æ•°æ®ç±»å‹",
                    ["bf16", "fp8"]
                )
                
            with train_col1_2:
                tp_size = st.number_input(
                    "TPå¤§å°",
                    min_value=1,
                    value=st.session_state.get('tp_size', 1),
                    key='tp_size'
                )
            
                ep_size = st.number_input(
                    "EPå¤§å°",
                    min_value=1,
                    value=st.session_state.get('ep_size', 8),
                    key='ep_size'
                )
                
                pp_size = st.number_input(
                    "PPå¤§å°",
                    min_value=1,
                    value=st.session_state.get('pp_size', 1),
                    key='pp_size'
                )
                with st.expander("PPå±‚æ•°é«˜çº§é€‰é¡¹"):
                    num_layers_in_first_pipeline_stage = st.number_input(
                        "é¦–ä¸ª Pipeline Stageçš„å±‚æ•°",
                        min_value=-1,
                        value=st.session_state.get('num_layers_in_first_pipeline_stage', -1),
                        key='num_layers_in_first_pipeline_stage',
                        help="å¦‚æœä¸º-1ï¼Œåˆ™è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼"
                    )
                    num_layers_in_last_pipeline_stage = st.number_input(
                        "æœ€åä¸€ä¸ª Pipeline Stageçš„å±‚æ•°",
                        min_value=-1,
                        value=st.session_state.get('num_layers_in_last_pipeline_stage', -1),
                        key='num_layers_in_last_pipeline_stage',
                        help="å¦‚æœä¸º-1ï¼Œåˆ™è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼"
                    )
                world_size = st.number_input(
                    "å¡æ•°",
                    min_value=1,
                    value=st.session_state.get('world_size', 8),
                    key='world_size'
                )
                
            if 'previous_model' not in st.session_state:
                st.session_state.previous_model = main_model

            if st.session_state.previous_model != main_model:
                model_config = perf_model.model_config
                # å°†dataclassè½¬æ¢ä¸ºå­—å…¸å¹¶æ‰¹é‡æ›´æ–°
                config_dict = model_config.__dict__
                for key, value in config_dict.items():
                    if not key.startswith('_'):  # è·³è¿‡ç§æœ‰å±æ€§
                        st.session_state[key] = value
                st.session_state.previous_model = main_model
                st.rerun()

            with st.expander("ğŸ”½ æ¨¡å‹å‚æ•°é…ç½®"):#â†“
                model_col1_1, model_col1_2, model_col1_3 = st.columns(3)
                with model_col1_1:
                    # noraml config
                    st.markdown("##### ğŸ¯å¸¸è§„å‚æ•°")
                    layer_num = st.number_input(
                        "å±‚æ•°",
                        min_value=1,
                        value=st.session_state.get('layer_num', perf_model.model_config.layer_num),
                        key='layer_num'
                    )
                    hidden_size = st.number_input(
                        "hidden_size",
                        min_value=1,
                        value=st.session_state.get('hidden_size', perf_model.model_config.hidden_size),
                        key='hidden_size'
                    )
                    intermediate_size = st.number_input(
                        "intermediate_size",
                        min_value=1,
                        value=st.session_state.get('intermediate_size', perf_model.model_config.intermediate_size),
                        key='intermediate_size'
                    )
                    vocab_size = st.number_input(
                        "vocab_size",
                        min_value=1,
                        value=st.session_state.get('vocab_size', perf_model.model_config.vocab_size),
                        key='vocab_size'
                    )
                with model_col1_2:
                    # attention related
                    st.markdown("##### ğŸ‘ï¸Attentionå‚æ•°")
                    head_num = st.number_input(
                        "head_num",
                        min_value=1,
                        value=st.session_state.get('head_num', perf_model.model_config.head_num),
                        key='head_num'
                    )
                    kv_head_num = st.number_input(
                        "kv_head_num",
                        min_value=1,
                        value=st.session_state.get('kv_head_num', perf_model.model_config.kv_head_num),
                        key='kv_head_num'
                    )
                    head_size = st.number_input(
                        "head_size",
                        min_value=1,
                        value=st.session_state.get('head_size', perf_model.model_config.head_size),
                        key='head_size'
                    )
                    if perf_model.model_config.attention_type == 'mla':
                        qk_head_dim = st.number_input(
                            "qk_head_dim",
                            min_value=1,
                            value=st.session_state.get('qk_head_dim', perf_model.model_config.qk_head_dim),
                            key='qk_head_dim'
                        )
                        v_head_dim = st.number_input(
                            "v_head_dim",
                            min_value=1,
                            value=st.session_state.get('v_head_dim', perf_model.model_config.v_head_dim),
                            key='v_head_dim'
                        )
                        qk_pos_emb_head_dim = st.number_input(
                            "qk_pose_emb_head_dim",
                            min_value=1,
                            value=st.session_state.get('qk_pose_emb_head_dim', perf_model.model_config.qk_pos_emb_head_dim),
                            key='qk_pose_emb_head_dim'
                        )
                        q_lora_rank = st.number_input(
                            "q_lora_rank",
                            min_value=1,
                            value=st.session_state.get('q_lora_rank', perf_model.model_config.q_lora_rank),
                            key='q_lora_rank'
                        )
                        kv_lora_rank = st.number_input(
                            "kv_lora_rank",
                            min_value=1,
                            value=st.session_state.get('kv_lora_rank', perf_model.model_config.kv_lora_rank),
                            key='kv_lora_rank'
                        )
                if perf_model.model_config.model_type == 'moe':
                    with model_col1_3:
                        # moe related
                        st.markdown("##### ğŸ—ï¸Moeå‚æ•°")
                        dense_layers = st.number_input(
                            "dense_layers",
                            min_value=1,
                            value=st.session_state.get('dense_layers', perf_model.model_config.dense_layers),
                            key='dense_layers'
                        )
                        expert_num = st.number_input(
                            "expert_num",
                            min_value=1,
                            value=st.session_state.get('expert_num', perf_model.model_config.expert_num),
                            key='expert_num'
                        )
                        topk = st.number_input(
                            "topk",
                            min_value=1,
                            value=st.session_state.get('topk', perf_model.model_config.topk),
                            key='topk'
                        )
                        moe_ffn_hidden_size = st.number_input(
                            "moe_ffn_hidden_size",
                            min_value=1,
                            value=st.session_state.get('moe_ffn_hidden_size', perf_model.model_config.moe_ffn_hidden_size),
                            key='moe_ffn_hidden_size'
                        )
                        moe_shared_expert_intermediate_size = st.number_input(
                        "moe_shared_expert_intermediate_size",
                        min_value=1,
                        value=st.session_state.get('moe_shared_expert_intermediate_size', perf_model.model_config.moe_shared_expert_intermediate_size),
                        key='moe_shared_expert_intermediate_size'
                    )

            st.subheader("é‡è®¡ç®—å‚æ•°")#ğŸ”„
            col1_1, col1_2 = st.columns(2)
            with col1_1:
                recompute_granularity = st.selectbox(
                    "é‡è®¡ç®—ç²’åº¦",
                    options=[None, "selective_recompute", "full_recompute"],
                    format_func=lambda x: "æ— " if x is None else x,
                    key='recompute_granularity'
                )
                recompute_layer_num = st.number_input(
                    "é‡è®¡ç®—å±‚æ•°",
                    min_value=0,
                    value=st.session_state.get('recompute_layer_num', 0),
                    key='recompute_layer_num'
                )
            if recompute_granularity == "selective_recompute":
                with col1_2:
                    attn_recompute = st.checkbox(
                        "ATTENTIONé‡è®¡ç®—",
                        value=st.session_state.get('attn_recompute', False),
                        key='attn_recompute'
                    )
                    mla_rms_recompute = st.checkbox(
                        "MLA RMSé‡è®¡ç®—",
                        value=st.session_state.get('mla_rms_recompute', False),
                        key='mla_rms_recompute'
                    )
                    
                    mlp_recompute = st.checkbox(
                        "MLPé‡è®¡ç®—",
                        value=st.session_state.get('mlp_recompute', False),
                        key='mlp_recompute'
                    )
                    
                    mlp_rms_recompute = st.checkbox(
                        "MLP RMSé‡è®¡ç®—", 
                        value=st.session_state.get('mlp_rms_recompute', False),
                        key='mlp_rms_recompute'
                    )
            else:
                attn_recompute = False
                mla_rms_recompute = False
                mlp_recompute = False
                mlp_rms_recompute = False

    with col2:
        st.header("ğŸ“ˆ åˆ†æç»“æœ")
        
        # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„é…ç½®
        st.info(f"**ç¡¬ä»¶:** {main_hardware} | **æ¨¡å‹:** {main_model} | **å¹¶è¡Œ:TP{tp_size}+EP{ep_size}+PP{pp_size}+GPU{world_size}**")
        
        # å½“ç‚¹å‡»åˆ†ææŒ‰é’®æ—¶æ‰§è¡Œåˆ†æ
        if analyze_btn:
            with st.spinner("æ­£åœ¨åˆ†æé…ç½®..."):    
                try:
                    # 1. set model config, refer æ¨¡å‹å‚æ•°é…ç½®
                    ## normal model config
                    perf_model.model_config.layer_num = layer_num
                    perf_model.model_config.hidden_size = hidden_size
                    perf_model.model_config.intermediate_size = intermediate_size
                    perf_model.model_config.vocab_size = vocab_size
                    perf_model.strategy.dispatch_probs = True

                    ## attention model config
                    perf_model.model_config.head_num = head_num
                    perf_model.model_config.kv_head_num = kv_head_num
                    perf_model.model_config.head_size = head_size
                    if perf_model.model_config.attention_type == 'mla':
                        perf_model.model_config.qk_head_dim = qk_head_dim
                        perf_model.model_config.v_head_dim = v_head_dim
                        perf_model.model_config.qk_pos_emb_head_dim = qk_pos_emb_head_dim
                        perf_model.model_config.q_lora_rank = q_lora_rank
                        perf_model.model_config.kv_lora_rank = kv_lora_rank
                    if perf_model.model_config.model_type == 'moe':
                        perf_model.model_config.dense_layers = dense_layers
                        perf_model.model_config.expert_num = expert_num
                        perf_model.model_config.topk = topk
                        perf_model.model_config.moe_ffn_hidden_size = moe_ffn_hidden_size
                        perf_model.model_config.moe_shared_expert_intermediate_size = moe_shared_expert_intermediate_size

                    # 2. set bw and tflops
                    if 'high_intra_node' in perf_model.system.networks:
                        perf_model.system.networks['high_intra_node'].bandwidth.gbps = intra_network_bandwidth
                    if 'inter_node' in perf_model.system.networks:
                        perf_model.system.networks['inter_node'].bandwidth.gbps = inter_network_bandwidth
                    perf_model.system.accelerator.op['default'].tflops = compute_performance
                    perf_model.system.accelerator.op['matmul'].tflops = compute_performance
                    perf_model.system.accelerator.op['fp8_matmul'].tflops = compute_performance
                    perf_model.system.accelerator.op['sdp_fwd'].tflops = compute_performance
                    perf_model.system.accelerator.op['sdp_bwd'].tflops = compute_performance
                    perf_model.system.accelerator.op['group_matmul'].tflops = compute_performance
                    perf_model.system.accelerator.op['fp8_group_matmul'].tflops = compute_performance

                    perf_model.model_config.moe_pad_expert_input_to_capacity = True
                    # TODO(sherry): add op efficiency

                    # 3. set paralilel strategy
                    perf_model.strategy.seq_len = seq_len
                    perf_model.strategy.micro_batch_size = micro_batch_size
                    perf_model.strategy.tp_size = tp_size
                    perf_model.strategy.ep_size = ep_size
                    perf_model.strategy.pp_size = pp_size
                    perf_model.strategy.world_size = world_size
                    
                    if num_layers_in_last_pipeline_stage != -1:
                        perf_model.strategy.num_layers_in_last_pipeline_stage = num_layers_in_last_pipeline_stage
                    if num_layers_in_first_pipeline_stage != -1:
                        perf_model.strategy.num_layers_in_first_pipeline_stage = num_layers_in_first_pipeline_stage
                    perf_model.strategy.reset_global_batch_size(global_batch_size)


                    # 4. set recompute strategy
                    perf_model.strategy.enable_recompute = True
                    if recompute_granularity == 'full_recompute':
                        perf_model.strategy.recompute_granularity = 'full_block' 
                    elif recompute_granularity == 'selective_recompute':
                        perf_model.strategy.recompute_granularity = 'selective_recompute'
                    else:
                        perf_model.strategy.recompute_granularity = None
                    perf_model.strategy.recompute_layer_num = recompute_layer_num
                    perf_model.strategy.attn_recompute = attn_recompute
                    perf_model.strategy.mla_rms_recompute = mla_rms_recompute
                    perf_model.strategy.mlp_recompute = mlp_recompute
                    perf_model.strategy.mlp_rms_recompute = mlp_rms_recompute

                    # 5. set dtype
                    perf_model.strategy.dtype = 'bf16'
                    if dtype == "fp8":
                        perf_model.strategy.fp8 = True
                    
                    # 6. run estimate
                    perf_model.run_estimate()
                    result = perf_model.analysis()
                    mem_results = perf_model.analysis_mem().data
                    cost_results = perf_model.analysis_cost().data
                    
                    st.session_state.analysis_result = (result, mem_results, cost_results, perf_model.strategy)
                except Exception as e:
                    print(f"è¯„ä¼°æŠ¥é”™:{e}")
                    st.session_state.warnings = f"è¯„ä¼°æŠ¥é”™:{e}"
            
            # è­¦å‘Šä¿¡æ¯
            if 'warnings' in st.session_state:
                st.subheader("âš ï¸ è­¦å‘Šä¿¡æ¯")
                st.error(st.session_state.warnings)
                # åˆ é™¤è­¦å‘Šä¿¡æ¯
                del st.session_state.warnings
            elif 'analysis_result' in st.session_state: # æ˜¾ç¤ºåˆ†æç»“æœ
                try:
                    result, mem_results, cost_results, strategy = st.session_state.analysis_result
                    peak_mem = max(perf_model.get_pp_stage_peak_mem(mem_results, "peak_mem", False).values())
                    peak_mem_with_reserved = max(perf_model.get_pp_stage_peak_mem(mem_results, "peak_mem_with_reserved", False).values())
                    
                    has_missed_op_efficiency = len(perf_model.system.miss_efficiency) > 0
                    if has_missed_op_efficiency:
                        missed_op_efficiency = copy.deepcopy(perf_model.system.miss_efficiency)
                        perf_model.system.reset_record_info()
                        
                    overflow_memory = peak_mem_with_reserved/2**30 > perf_model.system.accelerator.mem_gbs
                    if overflow_memory or has_missed_op_efficiency:
                        st.subheader("ğŸ’¡ æç¤º/å»ºè®®") #â—ï¸
                        # st.warning(f"**å³°å€¼Reservedæ˜¾å­˜({HumanReadableSize(peak_mem_with_reserved)})è¶…è¿‡ç³»ç»Ÿæ˜¾å­˜é™åˆ¶({perf_model.system.accelerator.mem_gbs}GB), å»ºè®®å¢åŠ å¡æ•°æˆ–è°ƒæ•´å¹¶è¡Œç­–ç•¥ã€é‡è®¡ç®—ç­–ç•¥**")
                        warn_idx = 1
                        if overflow_memory:
                            st.markdown(
                                f'<p style="color:red;">âš ï¸ <strong>{warn_idx}. å³°å€¼Reservedæ˜¾å­˜({HumanReadableSize(peak_mem_with_reserved)})è¶…è¿‡ç³»ç»Ÿæ˜¾å­˜é™åˆ¶({perf_model.system.accelerator.mem_gbs}GB), å»ºè®®å¢åŠ å¡æ•°æˆ–è°ƒæ•´å¹¶è¡Œç­–ç•¥ã€é‡è®¡ç®—ç­–ç•¥</strong></p>',
                                unsafe_allow_html=True
                            )
                            warn_idx += 1
                        if has_missed_op_efficiency:
                            st.markdown(f'<p style="color:red;">âš ï¸ <strong>{warn_idx}. ä¸‹é¢çš„op shapeè®¡ç®—æ•ˆç‡ç¼ºå¤±,å¯èƒ½å½±å“è¯„ä¼°å‡†ç¡®åº¦,å»ºè®®é€šè¿‡opæµ‹è¯•è„šæœ¬è¡¥å…¨ç¼ºå¤±shapeçš„è®¡ç®—æ•ˆç‡</strong></p>',
                                unsafe_allow_html=True)
                            st.write(missed_op_efficiency)
                            warn_idx += 1

                    strategy:StrategyConfig  = strategy
                    # å…³é”®æŒ‡æ ‡
                    st.subheader("ğŸ“Š å…³é”®æŒ‡æ ‡")
                    metric_col1, metric_col2, metric_col3 = st.columns(3)
                    
                    with metric_col1:
                        st.metric("è®¡ç®—çš„å¡æ•°", strategy.world_size)
                        st.metric("å†…å­˜ä¼°è®¡" \
                        "(Peak Alloc)", f"{HumanReadableSize(peak_mem)}")
                        
                    with metric_col2:
                        st.metric("å®é™…å…¨å±€æ‰¹æ¬¡å¤§å°", strategy.global_batch_size)
                        st.metric("MFU", f"{cost_results['mfu_6nd_with_attn']*100:.2f}%")
                        
                    with metric_col3:
                        st.metric("Tokenååé‡(TGS)", f"{cost_results['throughput_per_accelerator']:.2f}")
                        st.metric("ç®—åŠ›ååé‡(TFLOPS)", f"{cost_results['throughput per GPU (TFLOP/s/GPU)']:.2f}")
                    
                    # å†…å­˜ç»†åˆ†
                    st.subheader("ğŸ’¾ å†…å­˜åˆ†æ")
                    if perf_model.strategy.pp_size == 1:
                        stages = ['first_stage']
                    elif perf_model.strategy.pp_size == 2:
                        stages = ['first_stage', 'last_stage']
                    elif perf_model.strategy.pp_size > 2:
                        stages = ['first_stage', 'middle_stage', 'last_stage']
                    pp_stage_labels = {
                        'first_stage': 'Pipelineå¹¶è¡Œç¬¬ä¸€é˜¶æ®µ',
                        'middle_stage': 'Pipelineå¹¶è¡Œä¸­é—´é˜¶æ®µ',
                        'last_stage': 'Pipelineå¹¶è¡Œæœ€åé˜¶æ®µ'
                    }
                    for stage in stages:
                        if perf_model.strategy.pp_size > 1:
                            context = st.expander(f"ğŸ”½ {pp_stage_labels[stage]}", expanded=True)
                        else:
                            context = st.container()
                        with context:#ğŸ“
                            if perf_model.strategy.pp_size > 1:
                                st.markdown(f"##### {pp_stage_labels[stage]}")
                                mem_result = mem_results[stage]
                            else:
                                mem_result = mem_results
                            mem_col1, mem_col2, mem_col3 = st.columns(3)
                            
                            with mem_col1:
                                st.metric("å‰å‘æ¿€æ´»å†…å­˜(å•Batch)", f"{mem_result['fwd_activation_cache_per_micro_batch']}")
                                st.metric("1F1Bå³°å€¼æ¿€æ´»å†…å­˜(å•Batch)", f"{mem_result['peak_activation_mem_in_1F1B']}")

                            with mem_col2:
                                st.metric("æ¨¡å‹å†…å­˜", f"{mem_result['model_mem']}")
                                with st.expander("ğŸ“Š æ¨¡å‹å†…å­˜ç»†åˆ†"):
                                    st.write(f"- MoEéƒ¨åˆ†: {mem_result['model_mem_detail']['moe']}")
                                    st.write(f"- Denseéƒ¨åˆ†: {mem_result['model_mem_detail']['dense']}")
                            with mem_col3:
                                st.metric("æ€»å³°å€¼Allocæ˜¾å­˜", f"{mem_result['peak_mem']}")
                                st.metric("æ€»å³°å€¼Reservedæ˜¾å­˜", f"{mem_result['peak_mem_with_reserved']}")
                    # é…ç½®éªŒè¯çŠ¶æ€
                    st.subheader("âœ… é…ç½®éªŒè¯")
                    st.success("é…ç½®éªŒè¯é€šè¿‡ âœ“")
                    
                    # st.warning("é…ç½®éªŒè¯æœªé€šè¿‡ âš ")
                    # st.subheader("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
                    # with mem_col2:
                    #     st.metric("æ¨¡å‹å†…å­˜", f"{analysis['model_memory_gb']} GB")
                    # with mem_col3:
                    #     st.metric("æ€»å†…å­˜", f"{analysis['memory_estimate_gb']} GB")
                    
                    
                    # æ˜¾ç¤ºresutltä¿¡æ¯
                    st.subheader("ğŸ“Š æ±‡æ€»ä¿¡æ¯")
                    st.write(result)
                    with st.expander("è¯¦ç»†é€šä¿¡å¸¦å®½"):
                        st.write(perf_model.system.real_comm_bw)
                    # ä¸‹è½½æŠ¥å‘Š
                    st.subheader("ğŸ“¥ ä¸‹è½½æŠ¥å‘Š")
                    zip_buffer = create_download_zip(perf_model, mem_results, cost_results)
                    st.download_button(
                        label="ä¸‹è½½åˆ†ææŠ¥å‘Š (ZIP)",
                        data=zip_buffer,
                        file_name=f"training_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                        mime="application/zip",
                        use_container_width=True
                    )
                    # del st.session_state.analysis_result
                except Exception as e:
                    print(f"åˆ†ææŠ¥é”™:{e}")
                    st.subheader("âš ï¸ è­¦å‘Šä¿¡æ¯")
                    st.error(f"åˆ†ææŠ¥é”™:{e}")
        else:
            st.info("è¯·ç‚¹å‡»'å¼€å§‹è¯„ä¼°é…ç½®'æŒ‰é’®å¼€å§‹åˆ†æ")

if __name__ == "__main__":
    main()